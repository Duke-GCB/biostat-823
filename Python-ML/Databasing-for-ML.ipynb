{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e54dafc",
   "metadata": {},
   "source": [
    "# Scalable and versatile databasing for ML\n",
    "\n",
    "Here we introduce Duckdb and Parquet, two tools that can be used to store and query data in a scalable and versatile way. We will show how to use them in Python and how to integrate them with other tools like Pandas.\n",
    "\n",
    "## Iris dataset in Pandas: recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce260a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path prefix for work directory as it might differ based on the environment\n",
    "from pathlib import Path\n",
    "WORK = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc40af-6f96-46ce-a130-1fbbcea722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3af85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnds2df(ds):\n",
    "    df = pd.DataFrame(data=ds.data, columns=ds.feature_names)\n",
    "    df['target'] = pd.Series(pd.Categorical.from_codes(ds.target,\n",
    "                                                       categories=ds.target_names))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ff27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_ds = datasets.load_iris()\n",
    "iris = sklearnds2df(iris_ds)\n",
    "iris.columns = iris.columns.str.removesuffix(\" (cm)\").str.replace(\" \",\"_\")\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82604487",
   "metadata": {},
   "source": [
    "### Case study: Two datasets with features to be combined\n",
    "\n",
    "A very common use-case in dataset preparation for ML is to combine features from two datasets into a single one.\n",
    "\n",
    "We simulate two datasets, one with the length and width features of the sepal, and another with the features of the iris petal, two different parts of the Iris flower. We then want to combine them into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_sepals = iris[['sepal_length','sepal_width','target']]\n",
    "iris_petals = iris[['petal_length','petal_width','target']]\n",
    "iris_sepals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(iris_sepals, iris_petals, how='inner', on='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a37213",
   "metadata": {},
   "source": [
    "`pandas.merge()` corresponds to a JOIN operation in SQL, and therefore needs a unique shared identifier on which to merge. As we can see, attempting to merge blocks of rows for the same category is na√Øve and won't yield the desired result.\n",
    "\n",
    "In Pandas, dataframes have an index, which can serve this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(iris_sepals, iris_petals, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06630cee",
   "metadata": {},
   "source": [
    "But not all dataframe implementations have an index. In facts, ones built for high performance and scalability, like Polars, don't have an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def sklearnds2pl(ds):\n",
    "    df = pl.DataFrame(data=ds.data, schema=[col.replace(\" (cm)\",\"\") for col in ds.feature_names])\n",
    "    targets = [ds.target_names[i] for i in ds.target]\n",
    "    return df.with_columns(pl.Series(targets).cast(pl.Categorical).alias('target'))\n",
    "sklearnds2pl(iris_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b97362",
   "metadata": {},
   "source": [
    "And in a more realistic scenario, the features in the two datasets may not be named distinctly. Let's create this case and give each dataset a unique (specimen or plant) ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_col_subset(df, prefix):\n",
    "    dfs = df.filter(regex=f\"^{prefix}\")\n",
    "    dfs = pd.concat([dfs, df.select_dtypes(include='category')], axis=1)\n",
    "    dfs.index = [f\"P{i:03d}\" for i in iris.index.values]\n",
    "    dfs.columns = dfs.columns.str.removeprefix(prefix)\n",
    "    return dfs.reset_index(names='ID')\n",
    "iris_sepals = create_col_subset(iris, 'sepal_')\n",
    "iris_petals = create_col_subset(iris, 'petal_')\n",
    "iris_sepals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(iris_sepals, iris_petals, on='ID', how='inner', suffixes=('_sepal','_petal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f8a4",
   "metadata": {},
   "source": [
    "The point isn't that this isn't possible with Pandas. The point is that we are using the wrong tool for the job. Pandas is great for data manipulation, but it's not a database. It's not designed to store and query data efficiently. It's designed to manipulate data efficiently. _(Adapted from a Copilot autocompletion, which says something about how common this problem is.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe001188",
   "metadata": {},
   "source": [
    "# The database way: Duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = duckdb.from_df(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e26c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "(type(db), db.shape, db.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1746709",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from db limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select min(sepal_width), max(sepal_width) from db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select target, avg(sepal_length), avg(sepal_width) from db group by target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75e73",
   "metadata": {},
   "source": [
    "### Duckdb can access dataframes directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680880a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select target, avg(sepal_length), avg(sepal_width) from iris group by target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4133f-0daa-4da2-9f31-272a51615c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select s.length as sepal_length, s.width as sepal_width \"\n",
    "           \"from iris_sepals as s \"\n",
    "           \"JOIN iris_petals USING (ID)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48344a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select s.ID as sID, p.ID as pID, s.length as sepal_length, s.width as sepal_width, \"\n",
    "           \"p.length as petal_length, p.width as petal_width, p.target as species \"\n",
    "           \"from iris_sepals as s join iris_petals as p on (s.ID = p.ID) \"\n",
    "           \"order by species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b9655",
   "metadata": {},
   "source": [
    "### Duckdb can read online datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709ee10",
   "metadata": {},
   "source": [
    "#### [HuggingFace](https://hf.co) (HF)\n",
    "\n",
    "- One of if not the most widely used ML commons for models and datasets\n",
    "- Datasets are easy to use and have a lot of datasets.\n",
    "\n",
    "Duckdb can read datasets directly from HF. (In fact, Pandas can, too.) We can start with the [Iris dataset from scikit-learn](https://hf.co/datasets/scikit-learn/iris/) on HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_csv(\"https://huggingface.co/datasets/scikit-learn/iris/resolve/main/Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580ced2-64a7-495b-94fa-74b64ed1d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from read_csv('https://huggingface.co/datasets/scikit-learn/iris/resolve/main/Iris.csv') where species = 'Iris-setosa'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e28abe",
   "metadata": {},
   "source": [
    "#### fsspec and HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cadd6-2e16-4765-aad1-d3933498491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "# just like a local file system ('file'), but with 'hf' for huggingface\n",
    "fsspec.filesystem('hf').ls('hf://datasets/scikit-learn/iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_csv(\"hf://datasets/scikit-learn/iris/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ce089",
   "metadata": {},
   "source": [
    "### Parquet format\n",
    "\n",
    "Parquet is a columnar storage format optimized for reading and writing very large datasets in chunks. It is fast and efficient, both in terms of storage and in terms of reading and writing, and it supports querying and filtering data efficiently without having to read (and thus download) the entire dataset.\n",
    "\n",
    "Major features include:\n",
    "- Hybrid between columnar and row-oriented storage: row groupw, within which data is stored in columns.\n",
    "- Compression: Parquet is compressed by default.\n",
    "- Metadata: Parquet stores metadata about the dataset, which can be used to optimize queries.\n",
    "- Partitioning: Parquet can be partitioned, which can make queries faster when only a subset of the data needs to be read.\n",
    "\n",
    "[![Michael Berk, Demystifying the Parquet File Format](https://miro.medium.com/v2/resize:fit:1024/format:webp/1*QEQJjtnDb3JQ2xqhzARZZw.png)](https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705)\n",
    "\n",
    "The following resources are useful for learning more about Parquet:\n",
    "- [Parquet documentation](https://parquet.apache.org/)\n",
    "- [Demystifying the Parquet File Format](https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705)\n",
    "- [Understanding the Parquet File Format: A Comprehensive Guide](https://medium.com/@siladityaghosh/understanding-the-parquet-file-format-a-comprehensive-guide-b06d2c4333db)\n",
    "\n",
    "#### Parquet on HF\n",
    "\n",
    "All datasets on HF have an auto-converted Parquet version. The dataset viewer on HF uses this. _(For larger datasets that aren't natively in Parquet format, the auto-converted Parquet version may be truncated.)_\n",
    "\n",
    "We can read the Iris dataset directly from HF in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850777c-3054-4f82-a895-2693744b6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = duckdb.read_parquet(\"hf://datasets/scikit-learn/iris@~parquet/default/train/0000.parquet\")\n",
    "pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e34ac-749a-4173-a352-b6435ba403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from pdb where species = 'Iris-setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32159020",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * \"\n",
    "           \"from read_parquet('hf://datasets/scikit-learn/iris@~parquet/default/train/0000.parquet') \"\n",
    "           \"where species = 'Iris-setosa'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fed16",
   "metadata": {},
   "source": [
    "### Parquet databases are very scalable\n",
    "\n",
    "As an example, a very large dataset is [PD12M](https://huggingface.co/datasets/Spawning/PD12M). It is 12.4M rows in total, sharded into many Parquet files, each of which is about 19MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef976680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening one Parquet file\n",
    "rel = duckdb.read_parquet('hf://datasets/Spawning/PD12M/metadata/pd12m.000.parquet')\n",
    "rel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select count(*) from rel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340aeec-754e-40d9-9a5c-c8c78c28e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening all Parquet files, i.e., the entire dataset\n",
    "rel = duckdb.read_parquet('hf://datasets/Spawning/PD12M/metadata/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a579066",
   "metadata": {},
   "source": [
    "We can immediately inspeact the schema of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24268675",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa641bd-aa40-4d3a-9bec-086c58186557",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select count(*) from rel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602c43a-9313-4da9-9fd2-6388944f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select mime_type, count(*) from rel \"\n",
    "           \"where width > 1024 and height > 768 \"\n",
    "           \"group by mime_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from rel where width > 1024 and height > 768 and mime_type = 'image/gif'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85625922",
   "metadata": {},
   "source": [
    "### Writing and reading Parquet\n",
    "\n",
    "We can use duckdb to write the Iris dataset to Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.from_df(iris).to_parquet(f'{WORK}/iris.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708045a",
   "metadata": {},
   "source": [
    "But current versions of Pandas can do so as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594437b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.to_parquet(WORK / 'iris-pd.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9bf7ee",
   "metadata": {},
   "source": [
    "#### Writing/reading a partitioned Parquet dataset\n",
    "\n",
    "The \"pedestrian\" way to write a Parquet file is to use the `pyarrow` library. This is in fact the library that Pandas uses under the hood by default to read and write Parquet files.\n",
    "\n",
    "The `partition_cols` argument is used to write the dataset in a partitioned format. This can allow for very efficient querying of the data if the filtering by a query aligns with the partitioning. (Note that Pandas accepts this parameter as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pa.Table.from_pandas(iris)\n",
    "pq.write_to_dataset(table, root_path=WORK / 'iris', partition_cols=['target'],\n",
    "                    existing_data_behavior='delete_matching')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ebeb2",
   "metadata": {},
   "source": [
    "The by-column value partitioning scheme ic common and is called hive-style partitioning.\n",
    "\n",
    "Both Pandas and Duckdb can read partitioned Parquet datasets, but the arguments differ slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8531990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(path=WORK / 'iris', partitioning='hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18315148",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_parquet(f'{WORK}/iris/**/*.parquet', hive_partitioning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a74ad5",
   "metadata": {},
   "source": [
    "Note that with hive partitioning, the partitioned columns are not included in the Parquet files. They are only present in the directory structure. (Though their metadata is present in the Parquet files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_parquet('iris/target=setosa/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4ea5f",
   "metadata": {},
   "source": [
    "#### Query Performance\n",
    "\n",
    "Duckdb and Parquet achieve query performance for filtering etc that can rival or significantly exceed that of a traditional dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f007312",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = duckdb.sql(\"select * from read_parquet('iris.parquet') where target = 'setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = iris.query(\"target == 'setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = iris.loc[iris.target == 'setosa',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_large = iris.sample(n=100_000, replace=True)\n",
    "iris_large.to_parquet(f'{WORK}/iris_large.parquet')\n",
    "iris_large.to_csv(f'{WORK}/iris_large.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9536ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10_000):\n",
    "    res = duckdb.sql(\"select * from read_parquet('iris_large.parquet') \"\n",
    "                     \"where target = 'setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfe96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10_000):\n",
    "    res = iris_large.loc[iris_large.target == 'setosa',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5514211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dbfile = f'{WORK}/iris_large.parquet'\n",
    "for i in range(30_000):\n",
    "    res = duckdb.sql(f\"select target, count(*) from read_parquet('{dbfile}') group by target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(30_000):\n",
    "    res = iris_large.groupby('target', observed=True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321c04d",
   "metadata": {},
   "source": [
    "There are also different algorithms available for compression (e.g. Gzip, Brotli, Zstd) and encoding (e.g. Delta, RLE, PLAIN, DICT). These could further optimize query performance and storage efficiency for specific use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee8098",
   "metadata": {},
   "source": [
    "#### Appending to a Parquet dataset\n",
    "\n",
    "In the OLAP notion, Parquet datasets are not designed for mutability. Hence, rows can't be updated or deleted. \n",
    "\n",
    "However, although rows can't simply be appended to a Parquet _file_, new rows _can_ be appended to a Parquet _dataset_, which can consist of multiple files:\n",
    "\n",
    "- We can simply write new rows to a new Parquet file and make sure it is included in the file glob passed to the `read_parquet()` function. (Usually this means it should be in the same directory as the existing Parquet files.)\n",
    "- We can also add new rows to an existing partitioned Parquet dataset. To allow existing files  but prevent overwriting them, we need to a combination of basename template and existing data behavior.\n",
    "\n",
    "Let's say we have 3 batches of data but we receive them not all at once but in 3 separate batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_shuffled = iris.sample(frac=1, replace=False)\n",
    "iris_b1 = iris_shuffled.iloc[:50]\n",
    "iris_b2 = iris_shuffled.iloc[50:100]\n",
    "iris_b3 = iris_shuffled.iloc[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526b72a6",
   "metadata": {},
   "source": [
    "Write the first batch (assuming we don't have the others yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed33d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_to_dataset(pa.Table.from_pandas(iris_b1, preserve_index=False),\n",
    "                    root_path=f'{WORK}/iris-b', partition_cols=['target'],\n",
    "                    basename_template='b1-{i}.parquet',\n",
    "                    existing_data_behavior='overwrite_or_ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db850ec",
   "metadata": {},
   "source": [
    "Then we can keep appending batches as we get them (or write all batches at once if we have them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d256c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, batch in enumerate([iris_b2, iris_b3], start=2):\n",
    "    pq.write_to_dataset(pa.Table.from_pandas(batch, preserve_index=False),\n",
    "                        root_path=f'{WORK}/iris-b', partition_cols=['target'],\n",
    "                        basename_template=f'b{b}'+'-{i}.parquet',\n",
    "                        existing_data_behavior='overwrite_or_ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca832844",
   "metadata": {},
   "source": [
    "How we query the dataset remains the same, regardless of whether the data was written all at once or in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8582c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"select * from read_parquet('{WORK}/iris-b/*/*.parquet', hive_partitioning=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9266c5",
   "metadata": {},
   "source": [
    "Changing the schema of a Parquet dataset (such as by adding columns) is difficult. The best way to approach this is to create separate Parquet files for each consistent data batch, then use `duckdb.sql()` to combine them (presumably using some kind of OUTER JOIN), and writing the result back to a Parquet dataset.\n",
    "\n",
    "Say, our first batch doesn't have the petal measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_b1.filter(regex=\"(sepal|target).*\").to_parquet(f'{WORK}/iris-c1.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f0d5a",
   "metadata": {},
   "source": [
    "Then combine with the other two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5aea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = duckdb.sql(\"select * from iris_b2 UNION ALL \"\n",
    "                 \"select * from iris_b3 UNION ALL \"\n",
    "                 \"select sepal_length, sepal_width, null, null, target \" +\n",
    "                 f\"from read_parquet('{WORK}/iris-c1.parquet')\")\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50452dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel.to_parquet(f'{WORK}/iris-c.parquet')\n",
    "# or alternatively as a partitioned dataset:\n",
    "pq.write_to_dataset(rel.arrow(), root_path=f'{WORK}/iris-c', partition_cols=['target'],\n",
    "                    existing_data_behavior='delete_matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"select * from read_parquet('{WORK}/iris-c.parquet')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"select target, \"\n",
    "           \"count(*) as num_rows, count(petal_length) as num_petal_measures \"\n",
    "           f\"from read_parquet('{WORK}/iris-c.parquet')\" +\n",
    "           \"group by target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"select * from read_parquet('{WORK}/iris-c/*/*.parquet', hive_partitioning=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f1509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
