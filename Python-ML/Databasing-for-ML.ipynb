{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e54dafc",
   "metadata": {},
   "source": [
    "# Scalable and versatile databasing for ML\n",
    "\n",
    "Here we introduce Duckdb and Parquet, two tools that can be used to store and query data in a scalable and versatile way. We will show how to use them in Python and how to integrate them with other tools like Pandas.\n",
    "\n",
    "## Iris dataset in Pandas: recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc40af-6f96-46ce-a130-1fbbcea722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3af85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnds2df(ds):\n",
    "    df = pd.DataFrame(data=ds.data, columns=ds.feature_names)\n",
    "    df['target'] = pd.Series(pd.Categorical.from_codes(ds.target,\n",
    "                                                       categories=ds.target_names))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ff27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_ds = datasets.load_iris()\n",
    "iris = sklearnds2df(iris_ds)\n",
    "iris.columns = iris.columns.str.removesuffix(\" (cm)\").str.replace(\" \",\"_\")\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82604487",
   "metadata": {},
   "source": [
    "### Case study: Two datasets with features to be combined\n",
    "\n",
    "A very common use-case in dataset preparation for ML is to combine features from two datasets into a single one.\n",
    "\n",
    "We simulate two datasets, one with the length and width features of the sepal, and another with the features of the iris petal, two different parts of the Iris flower. We then want to combine them into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_sepals = iris[['sepal_length','sepal_width','target']]\n",
    "iris_petals = iris[['petal_length','petal_width','target']]\n",
    "iris_sepals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(iris_sepals, iris_petals, how='inner', on='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a37213",
   "metadata": {},
   "source": [
    "`pandas.merge()` corresponds to a JOIN operation in SQL, and therefore needs a unique shared identifier on which to merge. As we can see, attempting to merge blocks of rows for the same category is na√Øve and won't yield the desired result.\n",
    "\n",
    "In Pandas, dataframes have an index, which can serve this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(iris_sepals, iris_petals, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06630cee",
   "metadata": {},
   "source": [
    "But not all dataframe implementations have an index. In facts, ones built for high performance and scalability, like Polars, don't have an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def sklearnds2pl(ds):\n",
    "    df = pl.DataFrame(data=ds.data, schema=[col.replace(\" (cm)\",\"\") for col in ds.feature_names])\n",
    "    targets = [ds.target_names[i] for i in ds.target]\n",
    "    return df.with_columns(pl.Series(targets).cast(pl.Categorical).alias('target'))\n",
    "sklearnds2pl(iris_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b97362",
   "metadata": {},
   "source": [
    "And in a more realistic scenario, the features in the two datasets may not be named distinctly. Let's create this case and give each dataset a unique (specimen or plant) ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_col_subset(df, prefix):\n",
    "    dfs = df.filter(regex=f\"^{prefix}\")\n",
    "    dfs = pd.concat([dfs, df.select_dtypes(include='category')], axis=1)\n",
    "    dfs.index = [f\"P{i:03d}\" for i in iris.index.values]\n",
    "    dfs.columns = dfs.columns.str.removeprefix(prefix)\n",
    "    return dfs.reset_index(names='ID')\n",
    "iris_sepals = create_col_subset(iris, 'sepal_')\n",
    "iris_petals = create_col_subset(iris, 'petal_')\n",
    "iris_sepals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(iris_sepals, iris_petals, on='ID', how='inner', suffixes=('_sepal','_petal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f8a4",
   "metadata": {},
   "source": [
    "The point isn't that this isn't possible with Pandas. The point is that we are using the wrong tool for the job. Pandas is great for data manipulation, but it's not a database. It's not designed to store and query data efficiently. It's designed to manipulate data efficiently. _(Adapted from a Copilot autocompletion, which says something about how common this problem is.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe001188",
   "metadata": {},
   "source": [
    "# The database way: Duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = duckdb.from_df(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e26c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "(type(db), db.shape, db.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1746709",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from db limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select min(sepal_width), max(sepal_width) from db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select target, avg(sepal_length), avg(sepal_width) from db group by target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75e73",
   "metadata": {},
   "source": [
    "### Duckdb can access dataframes directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680880a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select target, avg(sepal_length), avg(sepal_width) from iris group by target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4133f-0daa-4da2-9f31-272a51615c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select s.length as sepal_length, s.width as sepal_width \"\n",
    "           \"from iris_sepals as s \"\n",
    "           \"JOIN iris_petals USING (ID)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48344a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select s.ID as sID, p.ID as pID, s.length as sepal_length, s.width as sepal_width, \"\n",
    "           \"p.length as petal_length, p.width as petal_width, p.target as species \"\n",
    "           \"from iris_sepals as s join iris_petals as p on (s.ID = p.ID) \"\n",
    "           \"order by species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b9655",
   "metadata": {},
   "source": [
    "### Duckdb can read online datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709ee10",
   "metadata": {},
   "source": [
    "#### [HuggingFace](https://hf.co) (HF)\n",
    "\n",
    "- One of if not the most widely used ML commons for models and datasets\n",
    "- Datasets are easy to use and have a lot of datasets.\n",
    "\n",
    "Duckdb can read datasets directly from HF. (In fact, Pandas can, too.) We can start with the [Iris dataset from scikit-learn](https://hf.co/datasets/scikit-learn/iris/) on HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_csv(\"https://huggingface.co/datasets/scikit-learn/iris/resolve/main/Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580ced2-64a7-495b-94fa-74b64ed1d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from read_csv('https://huggingface.co/datasets/scikit-learn/iris/resolve/main/Iris.csv') where species = 'Iris-setosa'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e28abe",
   "metadata": {},
   "source": [
    "#### fsspec and HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cadd6-2e16-4765-aad1-d3933498491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "# just like a local file system ('file'), but with 'hf' for huggingface\n",
    "fsspec.filesystem('hf').ls('hf://datasets/scikit-learn/iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_csv(\"hf://datasets/scikit-learn/iris/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ce089",
   "metadata": {},
   "source": [
    "### Parquet format\n",
    "\n",
    "Parquet is a columnar storage format optimized for reading and writing very large datasets in chunks. It is fast and efficient, both in terms of storage and in terms of reading and writing, and it supports querying and filtering data efficiently without having to read (and thus download) the entire dataset.\n",
    "\n",
    "Major features include:\n",
    "- Hybrid between columnar and row-oriented storage: row groupw, within which data is stored in columns.\n",
    "- Compression: Parquet is compressed by default.\n",
    "- Metadata: Parquet stores metadata about the dataset, which can be used to optimize queries.\n",
    "- Partitioning: Parquet can be partitioned, which can make queries faster when only a subset of the data needs to be read.\n",
    "\n",
    "[![Michael Berk, Demystifying the Parquet File Format](https://miro.medium.com/v2/resize:fit:1024/format:webp/1*QEQJjtnDb3JQ2xqhzARZZw.png)](https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705)\n",
    "\n",
    "The following resources are useful for learning more about Parquet:\n",
    "- [Parquet documentation](https://parquet.apache.org/)\n",
    "- [Demystifying the Parquet File Format](https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705)\n",
    "- [Understanding the Parquet File Format: A Comprehensive Guide](https://medium.com/@siladityaghosh/understanding-the-parquet-file-format-a-comprehensive-guide-b06d2c4333db)\n",
    "\n",
    "#### Parquet on HF\n",
    "\n",
    "All datasets on HF have an auto-converted Parquet version. The dataset viewer on HF uses this. _(For larger datasets that aren't natively in Parquet format, the auto-converted Parquet version may be truncated.)_\n",
    "\n",
    "We can read the Iris dataset directly from HF in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850777c-3054-4f82-a895-2693744b6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = duckdb.read_parquet(\"hf://datasets/scikit-learn/iris@~parquet/default/train/0000.parquet\")\n",
    "pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e34ac-749a-4173-a352-b6435ba403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from pdb where species = 'Iris-setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32159020",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * \"\n",
    "           \"from read_parquet('hf://datasets/scikit-learn/iris@~parquet/default/train/0000.parquet') \"\n",
    "           \"where species = 'Iris-setosa'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fed16",
   "metadata": {},
   "source": [
    "### Parquet databases are very scalable\n",
    "\n",
    "As an example, a very large dataset is [PD12M](https://huggingface.co/datasets/Spawning/PD12M). It is 12.4M rows in total, sharded into many Parquet files, each of which is about 19MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef976680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening one Parquet file\n",
    "rel = duckdb.read_parquet('hf://datasets/Spawning/PD12M/metadata/pd12m.000.parquet')\n",
    "rel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select count(*) from rel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340aeec-754e-40d9-9a5c-c8c78c28e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening all Parquet files, i.e., the entire dataset\n",
    "rel = duckdb.read_parquet('hf://datasets/Spawning/PD12M/metadata/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a579066",
   "metadata": {},
   "source": [
    "We can immediately inspeact the schema of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24268675",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa641bd-aa40-4d3a-9bec-086c58186557",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select count(*) from rel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602c43a-9313-4da9-9fd2-6388944f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select mime_type, count(*) from rel \"\n",
    "           \"where width > 1024 and height > 768 \"\n",
    "           \"group by mime_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from rel where width > 1024 and height > 768 and mime_type = 'image/gif'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85625922",
   "metadata": {},
   "source": [
    "### Writing and reading Parquet\n",
    "\n",
    "We can use duckdb to write the Iris dataset to Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.from_df(iris).to_parquet('iris.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708045a",
   "metadata": {},
   "source": [
    "But current versions of Pandas can do so as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594437b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.to_parquet('iris-pd.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9bf7ee",
   "metadata": {},
   "source": [
    "#### Writing/reading a partitioned Parquet dataset\n",
    "\n",
    "The \"pedestrian\" way to write a Parquet file is to use the `pyarrow` library. This is in fact the library that Pandas uses under the hood by default to read and write Parquet files.\n",
    "\n",
    "The `partition_cols` argument is used to write the dataset in a partitioned format. This can allow for very efficient querying of the data if the filtering by a query aligns with the partitioning. (Note that Pandas accepts this parameter as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pa.Table.from_pandas(iris)\n",
    "pq.write_to_dataset(table, root_path='iris', partition_cols=['target'],\n",
    "                    existing_data_behavior='delete_matching')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ebeb2",
   "metadata": {},
   "source": [
    "The by-column value partitioning scheme ic common and is called hive-style partitioning.\n",
    "\n",
    "Both Pandas and Duckdb can read partitioned Parquet datasets, but the arguments differ slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8531990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(path='iris', partitioning='hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18315148",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_parquet('iris/*/*.parquet', hive_partitioning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a74ad5",
   "metadata": {},
   "source": [
    "Note that with hive partitioning, the partitioned columns are not included in the Parquet files. They are only present in the directory structure. (Though their metadata is present in the Parquet files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_parquet('iris/target=setosa/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4ea5f",
   "metadata": {},
   "source": [
    "#### Query Performance\n",
    "\n",
    "Duckdb and Parquet achieve query performance for filtering etc that can rival or significantly exceed that of a traditional dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f007312",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = duckdb.sql(\"select * from read_parquet('iris.parquet') where target = 'setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = iris.query(\"target == 'setosa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = iris.loc[iris.target == 'setosa',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_large = iris.sample(n=100_000, replace=True)\n",
    "iris_large.to_parquet('iris_large.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9536ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = duckdb.sql(\"select * from read_parquet('iris_large.parquet') \"\n",
    "                     \"where target = 'setosa'\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50_000):\n",
    "    res = iris_large.loc[iris_large.target == 'setosa',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd05a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
